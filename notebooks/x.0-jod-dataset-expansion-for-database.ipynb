{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Video game localizion prioritization tool proposal\n",
    "---\n",
    "\n",
    "My goal is to scrape and analyze data from the video game platform\n",
    "Steam in order to help studios or localization service providers\n",
    "choose which languages they should localize into in order to\n",
    "maximize their localization ROI. In the process of doing this, I will also collect as much potentially useful data as possible for integration into a postgres database that could support future analyses & projects.\n",
    "\n",
    "For the purpose of the present analysis, any relationships between comment counts and, genre/tags, price, and operating system could be instrumental in driving business\n",
    "decisions on the studio or language service provider level.\n",
    "\n",
    "The code below scrapes a search result\n",
    "page to gather cursory information about the game (after ascertaining whether or not we have already scraped that game), then visits each game's page individually to scrape additional data, then visits each game's page again in each of the languages available on Steam in order to count the number of comments in that language.\n",
    "\n",
    "All of this information is stored in a single DataFrame, saved to disk as a json. Sorting the columns into different tables will happen after ingestion into postgres. Future iterations of the scraper may include a pipeline directly into postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DS stuff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Trying not to get blocked while scraping by inputting\n",
    "# random delays between Get requests.\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import chardet\n",
    "\n",
    "# I needed some extra help locating specific parts within a\n",
    "# bs4 tag object, so I got this.\n",
    "import re\n",
    "\n",
    "# For file tracking when exporting files.\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "# I didn't end up using this one, but that might be because\n",
    "# I still have no idea what the eff I'm doing. Leaving it for\n",
    "# now in case I need it later.\n",
    "import requests\n",
    "\n",
    "# To help see if we have existing data or not.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable will determine how many games to scrape per notebook execution.\n",
    "# I'm putting it all the way up here to make it easier to find.\n",
    "games_to_scrape = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Learn about the page\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE ONLY WORKS IF YOUR STEAM SETTINGS ARE SET TO PAGINATED\n",
    "# SEARCH RESULTS, NOT INFINITE SCROLL.\n",
    "\n",
    "# This url is for the \"all products\" search with the result type\n",
    "# limited to \"Games\" (category1=998)\n",
    "url = \"https://store.steampowered.com/search/?category1=998\"\n",
    "html = urlopen(url)\n",
    "current_page_soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"search_result_row ds_collapse_flag\" data-ds-appid=\"730\" data-ds-crtrids=\"[4]\" data-ds-descids=\"[2,5]\" data-ds-itemkey=\"App_730\" data-ds-steam-deck-compat-handled=\"true\" data-ds-tagids=\"[1663,1774,3859,3878,19,5711,5055]\" data-gpnav=\"item\" data-search-page=\"1\" href=\"https://store.steampowered.com/app/730/CounterStrike_2/?snr=1_7_7_230_150_1\" onmouseout=\"HideGameHover( this, event, 'global_hover' )\" onmouseover=\"GameHover( this, event, 'global_hover', {&quot;type&quot;:&quot;app&quot;,&quot;id&quot;:730,&quot;public&quot;:1,&quot;v6&quot;:1} );\">\n",
      " <div class=\"col search_capsule\">\n",
      "  <img src=\"https://cdn.cloudflare.steamstatic.com/steam/apps/730/capsule_sm_120.jpg?t=1696513856\" srcset=\"https://cdn.cloudflare.steamstatic.com/steam/apps/730/capsule_sm_120.jpg?t=1696513856 1x, https://cdn.cloudflare.steamstatic.com/steam/apps/730/capsule_231x87.jpg?t=1696513856 2x\"/>\n",
      " </div>\n",
      " <div class=\"responsive_search_name_combined\">\n",
      "  <div class=\"col search_name ellipsis\">\n",
      "   <span class=\"title\">\n",
      "    Counter-Strike 2\n",
      "   </span>\n",
      "   <div>\n",
      "    <span class=\"platform_img win\">\n",
      "    </span>\n",
      "    <span class=\"platform_img linux\">\n",
      "    </span>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"col search_released responsive_secondrow\">\n",
      "   Aug 21, 2012\n",
      "  </div>\n",
      "  <div class=\"col search_reviewscore responsive_secondrow\">\n",
      "   <span class=\"search_review_summary positive\" data-tooltip-html=\"Very Positive&lt;br&gt;88% of the 7,661,197 user reviews for this game are positive.\">\n",
      "   </span>\n",
      "  </div>\n",
      "  <div class=\"col search_price_discount_combined responsive_secondrow\" data-price-final=\"1499\">\n",
      "   <div class=\"col search_discount_and_price responsive_secondrow\">\n",
      "    <div class=\"discount_block no_discount search_discount_block\">\n",
      "     <div class=\"discount_prices\">\n",
      "      <div class=\"discount_final_price free\">\n",
      "       Free\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      " </div>\n",
      " <div style=\"clear: left;\">\n",
      " </div>\n",
      "</a>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From looking at the whole page's HTML, I can tell which tag to call in order\n",
    "# to get the information relevant to only a single game.\n",
    "\n",
    "single_game_example = current_page_soup.find('a', class_='search_result_row ds_collapse_flag')\n",
    "\n",
    "print(single_game_example.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I learned the hard way that not all listings are identical. Most listings are for 'app's, but\n",
    "# some are for 'bundle's. Bundles have a slightly different leading 'a' tag - not different enough\n",
    "# that we need to use a different attribute to access them, but different enough that we need to\n",
    "# use different attributes to scrape some of the data.\n",
    "\n",
    "# Let's pull one up for reference.\n",
    "\n",
    "for listing in current_page_soup.find_all('a', class_='search_result_row ds_collapse_flag') :\n",
    "    if listing.has_attr('data-ds-bundle-data') :\n",
    "        print(listing.prettify())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Scrape the first set of data from the search results pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know what our soups will look like, we can write functions to do the scraping.\n",
    "# The first function will scrape all the relevant data off of the current results page.\n",
    "# The second function will programmatically switch to the next page of results.\n",
    "# Later, we will run both functions within a loop in order to scrape all results data\n",
    "# from all pages.\n",
    "\n",
    "# This is only the first round of scraping. Later, we will scrape more data from each\n",
    "# game's store page. Since that process is completely different, we will define new\n",
    "# functions for it later, after this round of scraping is complete.\n",
    "\n",
    "# Loop through the HTML blocks for each game and scrape the key info into a dictionary,\n",
    "# then add the dictionaries to the list.\n",
    "# I'm not cleaning up the data types at this point - I'm learning as I'm going, so I'm\n",
    "# prioritizing getting all the info I need into the df, and then working with data\n",
    "# types later either by doing operations on the df or re-writing some of this code.\n",
    "def scrape_current_page(current_page_soup) :\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes the soup of a paginated Steam search results page (NOT infinte scroll)\n",
    "    and scrapes the:\n",
    "    \n",
    "    title\n",
    "    release_date\n",
    "    positive_review_percent\n",
    "    number_of_reviews\n",
    "    price\n",
    "    game_page_link\n",
    "    type\n",
    "    app_id\n",
    "    \n",
    "    from every game on the page. It puts these values into dictionaries and appends them to\n",
    "    the list called \"games\". \n",
    "    \"\"\"\n",
    "\n",
    "    for listing in current_page_soup.find_all('a', class_='search_result_row ds_collapse_flag') :\n",
    "\n",
    "        # Create (or clean out) an empty dictionary to hold the new info.\n",
    "        game = {}\n",
    "\n",
    "        # Check if we have the specified number of games yet.\n",
    "        if len(games) == games_to_scrape :\n",
    "            return\n",
    "\n",
    "        # Listings on results pages can be one of two types - standalone games, or bundles.\n",
    "        # We only want to work with standalone games.\n",
    "        # Only apps have this tag in their listing.\n",
    "        if listing.has_attr('data-ds-appid') :\n",
    "            raw_app_id = listing.get('data-ds-appid')\n",
    "            # To exclude bundles, we'll skip any listing with multiple app_ids.\n",
    "            # Since the app_id is scraped as a string, lists of app_ids will have\n",
    "            # commas separating them, and we can identify them by those commas.\n",
    "            if \",\" in raw_app_id :\n",
    "                continue\n",
    "            \n",
    "            app_id = int(raw_app_id)\n",
    "\n",
    "            # Make sure we haven't already scraped this one.\n",
    "            if app_id not in already_scraped_app_ids.values :\n",
    "\n",
    "                game['app_id'] = app_id\n",
    "                    \n",
    "                # The title and release date seem to be at uniform locations in all listings.\n",
    "                game['title'] = listing.find('span', class_='title').get_text()\n",
    "                raw_date = listing.find('div', class_='col search_released responsive_secondrow').get_text()\n",
    "                try:\n",
    "                    raw_date = datetime.strptime(raw_date, \"%m/%d/%Y\")\n",
    "                    formatted_date = raw_date.strftime(\"%Y-%m-%d\")\n",
    "                    game['release_date'] = formatted_date\n",
    "                except:\n",
    "                    game['release_date'] = raw_date\n",
    "\n",
    "                # Not all games have reviws listed, so we have to account for code blocks that omit this part.\n",
    "                # I might eventually remove this part and scrape the review data from the individual game pages\n",
    "                # instead, since it seems to be more complete there. This is just proof of concept for now.\n",
    "                try:\n",
    "                    review_string = re.split('>| of|the | user', listing.find('div', class_='col search_reviewscore responsive_secondrow') \\\n",
    "                                                                .find('span').get('data-tooltip-html'))\n",
    "                    raw_review_percent = review_string[1][:-1]\n",
    "                    float_review_percent = int(raw_review_percent) / 100\n",
    "                    formatted_review_percent = round(float_review_percent, 2)\n",
    "                    game['positive_review_percent'] = formatted_review_percent\n",
    "                except:\n",
    "                    game['positive_review_percent'] = np.nan\n",
    "                \n",
    "                try: \n",
    "                    review_string = re.split('>| of|the | user', listing.find('div', class_='col search_reviewscore responsive_secondrow') \\\n",
    "                                                .find('span').get('data-tooltip-html'))\n",
    "                    raw_review_number = review_string[3].replace(',', '')\n",
    "                    formatted_review_number = int(raw_review_number)\n",
    "                    game['number_of_reviews'] = formatted_review_number\n",
    "                except: \n",
    "                    game['number_of_reviews'] = np.nan\n",
    "                \n",
    "                # Same for price - many unreleased games do not have price info, so we have to skip them.\n",
    "                # Some games have an original price and a discounted price listed, but for the time being\n",
    "                # I've decided to only go by original prices, so I'll default to that and only return\n",
    "                # a null value if no kind of price whatsoever is listed.\n",
    "                try: \n",
    "                    raw_price = listing.find('div', class_=\"discount_original_price\").get_text()\n",
    "                    bare_price = raw_price.replace(\"$\", \"\").replace(\",\", \"\").replace(\".\", \"\")\n",
    "                    formatted_price = int(bare_price)\n",
    "                    game['price'] = formatted_price\n",
    "                except:\n",
    "                    try:\n",
    "                        raw_price = listing.find('div', class_=\"discount_final_price\").get_text()\n",
    "                        bare_price = raw_price.replace(\"$\", \"\").replace(\",\", \"\").replace(\".\", \"\")\n",
    "                        formatted_price = int(bare_price)\n",
    "                        game['price'] = formatted_price\n",
    "                    except:\n",
    "                        game['price'] = np.nan\n",
    "\n",
    "                # Weirdly enough, not every game seems to have its own page.\n",
    "                try:\n",
    "                    game['game_page_link'] = listing.get('href')\n",
    "                except:\n",
    "                    game['game_page_link'] = 'Failed'\n",
    "\n",
    "                # Now we grab the tags, which will be a major feature in our analysis.\n",
    "                try :\n",
    "                    raw_tags = listing.get('data-ds-tagids')\n",
    "                    formatted_tags = raw_tags.strip('[]').split(',')\n",
    "                    game['tags'] = formatted_tags\n",
    "                except :\n",
    "                    game['tags'] = 'Failed'\n",
    "\n",
    "                # Add the current date as a reference for future generations (and versioning).\n",
    "                todays_date = datetime.now()\n",
    "                game['date_scraped'] = todays_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                # Now we add this dict to the list, rinse and repeat.\n",
    "                games.append(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the function that determines if there is a next page of\n",
    "# results, or if we're already at the last page.\n",
    "\n",
    "def get_next_page_url(current_page_soup) :\n",
    "\n",
    "        \"\"\"\n",
    "        This function takes the soup of a paginated Steam search results page (NOT infinte scroll)\n",
    "        and determines whether it is the last page of results.\n",
    "\n",
    "        If it is not the last page, the URL of the next page is stored in \"next_link\".\n",
    "\n",
    "        If it is the last page, \"next_link\" will be set to False.\n",
    "        \"\"\"\n",
    "\n",
    "        # First, we check to make sure there IS a next page. We can tell by looking\n",
    "        # at the 'pagebtn' tags.\n",
    "        pagebtn_tags = current_page_soup.find_all('a', class_='pagebtn')\n",
    "\n",
    "        # This is the variable that we will use to store the next link, or set it to\n",
    "        # False to let the loop know that we're done scraping.\n",
    "        global next_link\n",
    "\n",
    "        # If it is any of the middle pages, there will be two pagebtn tags.\n",
    "        # The link we need is in side the pagebtn tag that displays the text '>'.\n",
    "\n",
    "        # After a while this loop fails because we get an empty pagebtn tag, which\n",
    "        # means that the page failed to load altogether. I will assume this is because\n",
    "        # the request timed out, and build in an additional delay to deal with that\n",
    "        # eventuality.\n",
    "        loops = 0\n",
    "        delay = 0\n",
    "\n",
    "        while loops < 5 :\n",
    "                try :\n",
    "                        if len(pagebtn_tags) == 2 :\n",
    "                                next_link = pagebtn_tags[1].get('href')\n",
    "\n",
    "                        # If there is only one pagebtn tag, that means we're on the first page or the \n",
    "                        # last page. If it's the first page, then the pagebtn tag will contain the\n",
    "                        # character '>'.\n",
    "                        elif pagebtn_tags[0].get_text() == \">\" :\n",
    "                                next_link = pagebtn_tags[0].get('href')\n",
    "\n",
    "                        # If neither of the above conditions are met, then we're on the last page and\n",
    "                        # we can set \"next_link\" to False, triggering the loop to stop scraping.\n",
    "                        else :\n",
    "                                next_link = False\n",
    "\n",
    "                        # We will use loops=100 as the indicator that the scrape was successful.\n",
    "                        loops = 100\n",
    "\n",
    "                except :\n",
    "                        # If the scrape was unsuccessful, we will try again 4 more times, with\n",
    "                        # an increasingly log delay between each attempt.\n",
    "                        delay += 10\n",
    "                        print(\"Search page failure \"+str(delay//10)+\"/5...\")\n",
    "                        time.sleep(delay)\n",
    "                        loops += 1\n",
    "        \n",
    "        if loops != 100 :\n",
    "                print('Failed to parse next_link on search results page:')\n",
    "                print(next_link)\n",
    "                next_link = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our functions, we'll iterate over them to scrape the data.\n",
    "\n",
    "# Set the first url to be processed to the first page of search results.\n",
    "next_link = url\n",
    "\n",
    "# Create the list that will hold the dictionaries of game info & errors.\n",
    "games = []\n",
    "\n",
    "# Now we decide how many results we want. \n",
    "# \n",
    "# The main constraint here is time - since\n",
    "# we don't want to get IP banned, we'll have set delay between each get request.\n",
    "# This isn't so important for this loop, since we can get 25 games in one get request.\n",
    "# However, later we'll be going through the games' pages one-by-one, and in some cases\n",
    "# we'll have to do 10 different get requests per game to scrape language-specific data.\n",
    "# Therefore, adding 1 game adds at least 11 get requests & delays to our process.\n",
    "# (I ended up scraping for over 10 hours.)\n",
    "#\n",
    "# Will only limit to inteverals of 25 (as there are 25 results per page).\n",
    "# If games_to_scrape is greater than the number of games in the search results, then\n",
    "# the the will automatically stop trying to scrape when it reaches the end of the\n",
    "# final page of results, because get_next_page_url will set the next_link variable to False.\n",
    "\n",
    "# Since we want to update this database from time to time, let's devise a way to scrape only\n",
    "# game data that is not already in the .json file that holds our master list. We'll do this\n",
    "# by pulling the app_ids from that file (if it exists) and pulling out only the app_ids to\n",
    "# check against.\n",
    "try :\n",
    "    check_df = pd.read_json('../data/raw/0 - Scraped Games DF.json', orient='records')\n",
    "    already_scraped_app_ids = check_df['app_id']\n",
    "except :\n",
    "    print('No scraped games detected. Scraping from scratch.')\n",
    "    already_scraped_app_ids = pd.Series()\n",
    "\n",
    "# Now, loop. Keep scraping as long as our games list is shorter than the games_to_scrape var.\n",
    "while len(games) < games_to_scrape :\n",
    "    \n",
    "    # Soup up the page in question.\n",
    "    html = urlopen(next_link)\n",
    "    current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Scrape that page.\n",
    "    scrape_current_page(current_page_soup)\n",
    "\n",
    "    # Set \"next_link\" to the next URL we want to scrape.\n",
    "    get_next_page_url(current_page_soup)\n",
    "\n",
    "    # Include a random delay to prevent getting IP blocked.\n",
    "    interval = 1 + random.random() * 0.5\n",
    "    time.sleep(interval)\n",
    "\n",
    "    if next_link == False :\n",
    "        print('Fewer than '+str(games_to_scrape)+' games in the available search results.')\n",
    "        if len(games) == 0 :\n",
    "            raise UserWarning('No games scraped from search results. Cannot continue.')\n",
    "\n",
    "print(str(len(games))+' games scraped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame it and check.\n",
    "scraped_search_results_df = pd.DataFrame(games)\n",
    "\n",
    "# This results in some duplicates - sometimes different versions of the game have the same app id.\n",
    "# Because we're interested in the relative ration of comment frequencies, not in the total number\n",
    "# of games or total number of comments, we can safely drop duplicates even if they have different\n",
    "# comments.\n",
    "# Since we already excluded app_ids that are duplicates with our previously-scraped files, now we\n",
    "# must remove duplicates that might exist within that last scrape.\n",
    "scraped_search_results_df = scraped_search_results_df.drop_duplicates(subset='app_id', keep='first')\n",
    "scraped_search_results_df = scraped_search_results_df.reset_index(drop=True)\n",
    "\n",
    "# Save this as a json to safeguard against crashes - running this scraper takes hours.\n",
    "scraped_search_results_df.to_json('../data/raw/Scraped Search Results.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Scrape additional data for each game from its individual game page\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to use the URLs we just scraped to go through the pages\n",
    "# one-by-one and scrape more data.\n",
    "\n",
    "# We'll put all this data in a completely different df, then join them\n",
    "# when we're done on app_id.\n",
    "def scrape_game_page_data(current_page_soup, app_id) :\n",
    "\n",
    "    \"\"\"\n",
    "    This function scrapes info from all the individual games pages\n",
    "    currently referenced in games_info_df. We put the info in a dict\n",
    "    \"game\", then append it to \"games_extend_list\".\n",
    "    \n",
    "    Later, we will turn that list into another df and merge it to\n",
    "    games_info_df on index.\n",
    "\n",
    "    Scraped information is:\n",
    "\n",
    "    app_id\n",
    "    developer\n",
    "    publisher\n",
    "    description\n",
    "    interface_languages\n",
    "    full_audio_languages\n",
    "    subtitles_languages\n",
    "    english     <-- the number of user comments in English\n",
    "    \"\"\"\n",
    "    # For bugfixing\n",
    "    global touched_ids\n",
    "    \n",
    "    # Create/clear out the dictionary.\n",
    "    game = {}\n",
    "\n",
    "    game[\"app_id\"] = app_id\n",
    "    touched_ids.append(game['app_id'])\n",
    "\n",
    "    # We can get the developer and publisher from the same code block.\n",
    "    try :\n",
    "        code_block = current_page_soup.find('div', attrs={'id':'appHeaderGridContainer'})\n",
    "    except :\n",
    "        pass\n",
    "\n",
    "    # The developer name is at a fixed location.\n",
    "    try:\n",
    "        raw_string = code_block.find('div', class_='grid_content').get_text()\n",
    "        # Don't know why it always brings in a newline at the beginning of the string. and a\n",
    "        # space at the end. Let's take those out.\n",
    "        formatted_string = raw_string[1:-1]\n",
    "        formatted_list = formatted_string.split(',')\n",
    "        game['developer'] = formatted_list\n",
    "    except :\n",
    "        game['developer'] = None\n",
    "\n",
    "    # The publisher name is also at a fixed location. Not every game has a publisher, though.\n",
    "    try :\n",
    "        raw_string = code_block.find('div', class_='grid_label', string='Publisher').find_next('a').get_text()\n",
    "        formatted_list = raw_string.split(',')\n",
    "        game['publisher'] = formatted_list\n",
    "    except :\n",
    "        game['publisher'] = None\n",
    "\n",
    "    # Descriptions are at a fixed location.\n",
    "    try:\n",
    "        game['description'] = current_page_soup.find('meta', attrs={'name':'Description'}).get('content')\n",
    "    except :\n",
    "        game['description'] = 'Failed'\n",
    "\n",
    "    # The languages are listed as rows of a table.\n",
    "    # There are three different ways languages can be implemented in the game.\n",
    "    # As we look through the table, we'll store the languages in separate lists.\n",
    "    interface_languages = []\n",
    "    full_audio_languages = []\n",
    "    subtitles_languages = []\n",
    "    language_types = [interface_languages, full_audio_languages, subtitles_languages]\n",
    "\n",
    "    # The source code is compex so let's isolate the relevant block for safety.\n",
    "    try :\n",
    "        languages_code_block = current_page_soup.find('table', class_='game_language_options')\n",
    "    # I'll leave a note for myself to help with bugfixing if needed.\n",
    "    except :\n",
    "        language_types[0] = 'Did not find code block'\n",
    "        \n",
    "    # Each \"row\" of the table is separated by a re tag. However, there's an extra\n",
    "    # tr tag at the beginning of languages_code_block that I couldn't find a better\n",
    "    # way to work around - since it has no text, it'll throw an error on .get_text,\n",
    "    # so we can just try/except our way out of it.\n",
    "    try :\n",
    "        for row in languages_code_block.find_all('tr', class_='') :\n",
    "            try :\n",
    "                current_language = row.find('td', class_='ellipsis').get_text()\n",
    "                # The text has a lot of formatting in it. No more!\n",
    "                current_language = re.sub('\\t|\\n|\\r', '', current_language)\n",
    "\n",
    "                # The code block represents each cell of the row with a td class='checkcol'\n",
    "                # tag. In order, the three cells of each row are interface, full audio,\n",
    "                # and subtitles. If the language of that row does not have one of those\n",
    "                # services, then there will be no more code inside the tags. If it does,\n",
    "                # then there will be a \"span\" tag in there along with a checkmark.\n",
    "\n",
    "                # Since the three types of language services are always in order,\n",
    "                # we can basically use 'counter' to iterate through the list of lists\n",
    "                # of language service types and only append the name of the language\n",
    "                # if that section of code has the \"span\" tag that indicates a checkmark.\n",
    "                counter = 0\n",
    "                for column in row.find_all('td', class_='checkcol') :\n",
    "                    if column.find('span') :\n",
    "                        language_types[counter].append(current_language)\n",
    "                    counter += 1\n",
    "            except :\n",
    "                pass\n",
    "    # For bugfixing.\n",
    "    except :\n",
    "        language_types[0] = 'Found code block, failed to parse within code block'\n",
    "\n",
    "\n",
    "    # Now we add the lists to our dictionary. We can access the lists via\n",
    "    # the index of the language_types list of lists.\n",
    "    game['interface_languages'] = language_types[0]\n",
    "    game['full_audio_languages'] = language_types[1]\n",
    "    game['subtitles_languages'] = language_types[2]\n",
    "\n",
    "    # I would love to have rating data available for the games, but Steam does not\n",
    "    # present it systematically (probably because so many games are not rated,\n",
    "    # and because there are different rating systems.)\n",
    "    # Maybe someday.\n",
    "    # game['rating'] = PG, Mature Audiences, etc...\n",
    "\n",
    "    # Now we get the number of reviews that are in English.\n",
    "    # To get the numbers for other languages, we'll have to modify the URL parameters\n",
    "    # and get the page again, so that'll be a big ol'loop that we'll do later.\n",
    "    try:\n",
    "        raw_english_comments = current_page_soup.find('label', attrs={'for':'review_language_mine'}) \\\n",
    "                                                    .find_next('span', class_='user_reviews_count').get_text()\n",
    "        formatted_english_comments = int(raw_english_comments.replace(',', '').strip('()'))\n",
    "        game['english'] = formatted_english_comments\n",
    "                                                            \n",
    "    except:\n",
    "        game['english'] = 0\n",
    "\n",
    "    # Rinse and repeat.\n",
    "    games_extend_list.append(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm declaring/cleaning out the list in a different cell because I hit a lot of \n",
    "# exceptions while testing this, and I didn't want to accidentally clean out all\n",
    "# my previous hard work each time I made a fix and continued the process. \n",
    "games_extend_list = []\n",
    "\n",
    "# Since running the following cell requires repeated get requests and sleep intervals,\n",
    "# and since many failures tend to happen 20 minutes or more into the process,\n",
    "# we can build in a ticker that keeps track of how far we got LAST time.\n",
    "# Then, after we debug, we can start right over from where we left off. \n",
    "ticker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bugfixing.\n",
    "touched_ids = []\n",
    "\n",
    "# Now we loop over all all app_ids in the df we created earlier.\n",
    "for index, row in scraped_search_results_df.iterrows() :\n",
    "    \n",
    "    # This is for bugfixing. If the loop throws an exception, I can use the ticker\n",
    "    # variable to quickly pick up where we left off.\n",
    "    if index == ticker :\n",
    "        # Soup up the page.\n",
    "        url = row['game_page_link']\n",
    "        html = urlopen(url)\n",
    "        current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Scrape the page.\n",
    "        scrape_game_page_data(current_page_soup, row['app_id'])\n",
    "\n",
    "        # Include a random delay to prevent getting IP blocked.\n",
    "        interval = 1 + random.random() * 0.5\n",
    "        time.sleep(interval)\n",
    "        \n",
    "        # If the loop throws an exception on a game, 'ticker' will thus be equal\n",
    "        # to that game's index in the df, and I can go see what the problem was.\n",
    "        ticker = index + 1\n",
    "        \n",
    "\n",
    "# Turn the new list of dicts into a new df.\n",
    "scraped_game_pages_df = pd.DataFrame(games_extend_list)\n",
    "scraped_game_pages_df.to_json('../data/raw/Scraped Game Pages.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20 entries, 0 to 19\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   app_id                   20 non-null     int64  \n",
      " 1   title                    20 non-null     object \n",
      " 2   release_date             20 non-null     object \n",
      " 3   positive_review_percent  19 non-null     float64\n",
      " 4   number_of_reviews        19 non-null     float64\n",
      " 5   price                    17 non-null     float64\n",
      " 6   game_page_link           20 non-null     object \n",
      " 7   tags                     20 non-null     object \n",
      " 8   date_scraped             20 non-null     object \n",
      " 9   developer                20 non-null     object \n",
      " 10  publisher                20 non-null     object \n",
      " 11  description              20 non-null     object \n",
      " 12  interface_languages      20 non-null     object \n",
      " 13  full_audio_languages     20 non-null     object \n",
      " 14  subtitles_languages      20 non-null     object \n",
      " 15  english                  20 non-null     int64  \n",
      "dtypes: float64(3), int64(2), object(11)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Now we join our dataframes to create our core dataset.\n",
    "# I say \"core,\" even though our all-important label has yet to be scraped.\n",
    "# Bear with me. I'm new at this.\n",
    "joined_games_df = pd.merge(scraped_search_results_df, scraped_game_pages_df, on=\"app_id\", how='inner')\n",
    "joined_games_df.to_json('../data/raw/Joined Games DF.json', orient='records')\n",
    "joined_games_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Scrape the number of comments in each language from the games' pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'top_10_languages' (list)\n"
     ]
    }
   ],
   "source": [
    "# Now we begin the task of getting all the comment counts for each different language.\n",
    "# Since this process requires a huge amount of get requests/time, we'll limit our exploration\n",
    "# to the 10 most common languages for game localization (assuming the source text is English).\n",
    "\n",
    "# Here's a list of all the language codes on Steam, for good measure.\n",
    "# Don't know if we'll use it, but here it is.\n",
    "all_languages = ['schinese', 'tchinese', 'japanese', 'koreana', 'thai', 'bulgarian', 'czech', 'danish', \\\n",
    "                 'german', 'english', 'spanish', 'latam', 'greek', 'french', 'italian', 'indonesian', \\\n",
    "                 'hungarian', 'dutch', 'norwegian', 'polish', 'portugese', 'brazilian', 'romanian', \\\n",
    "                 'russian', 'finnish', 'swedish', 'turkish', 'vietnamese', 'ukranian']\n",
    "\n",
    "# For some languages, steam displays 'comments in my language' as including English. Let's make a list\n",
    "# of them so that we can correct for this.\n",
    "languages_counted_with_english = ['german', 'danish', 'greek', 'dutch', 'norwegian', 'finnish', 'swedish']\n",
    "\n",
    "# Since we already have the EN comment counts, let's make a list that excludes EN for future\n",
    "# scraping. We can also remove all the languages that don't have their own distinct counts.\n",
    "all_counted_non_english_languages = all_languages.copy()\n",
    "all_counted_non_english_languages.remove('english')\n",
    "all_counted_non_english_languages.remove('portugese')\n",
    "all_counted_non_english_languages.remove('ukranian')\n",
    "\n",
    "# These are the generally-accepted top 10 languages to localize into from EN.\n",
    "# The count of EN comments is important for our analysis, but it's already in the df.\n",
    "# No idea why they put an a on the end of Korean.\n",
    "top_10_languages = ['german', 'french', 'spanish', 'brazilian', 'russian', 'italian', 'schinese', \\\n",
    "                    'japanese', 'koreana', 'polish']\n",
    "\n",
    "%store top_10_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build a function that will find the number of reviews in a given language for a given game.\n",
    "# This function will iterate through our df (using the first one, which is also the smallest, for\n",
    "# good measure), creating a new column for the language and filling the value with the number.\n",
    "app_comment_languages = []\n",
    "single_app_comment_languages = {}\n",
    "\n",
    "# We'll need this later. Just trust me.\n",
    "en_comment_counts_by_app_id = scraped_game_pages_df.set_index('app_id')['english']\n",
    "\n",
    "\n",
    "def comments_in_all_languages(app_id, languages) :\n",
    "    \"\"\"\n",
    "    Takes a Steam app id and a list of languages (as spelled in Steam's html)\n",
    "    and creates a dictionary, then appends that dictionary to a list.\n",
    "\n",
    "    Intended to be iterated over.\n",
    "\n",
    "    The first key in the dictionary is \"app id\", and the value is the app id.\n",
    "\n",
    "    The rest of the keys are the names of the languages, and the values are\n",
    "    the number of comments on that game/app's page that are in that language.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the dict is empty at the beginning of each loop.\n",
    "    single_app_comment_languages = {}\n",
    "    \n",
    "    # Store the app_id in the dict.\n",
    "    single_app_comment_languages['app_id'] = app_id\n",
    "\n",
    "    # Soup up the game's page in the current language.\n",
    "    for language in languages :\n",
    "        url = 'https://store.steampowered.com/app/'+str(app_id)+'/?l='+language\n",
    "        html = urlopen(url)\n",
    "        current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # There are 2 types of game page source code, used on games with different language settings.\n",
    "        # We'll try the most common one first, then try to execute the other type if this throws an exception.\n",
    "        try :\n",
    "            raw_comment_count = current_page_soup.find('label', attrs={'for':'review_language_mine'}).find_next('span').get_text()\n",
    "            formatted_comment_count = int(raw_comment_count.replace(',', '').strip('()'))\n",
    "            # Some languages display comment counts that are ADDED TO the EN count.\n",
    "            # Let's remove the EN value from them before moving on.\n",
    "            if language in languages_counted_with_english :\n",
    "                formatted_comment_count = formatted_comment_count - en_comment_counts_by_app_id[app_id]\n",
    "            single_app_comment_languages[language] = formatted_comment_count\n",
    "        \n",
    "        # If that's no good, we try scraping the other way.\n",
    "        # The 'other way' can't be scraped effectively by urlopen(), so we'll use requests.get() instead.\n",
    "        except :\n",
    "            try :\n",
    "                url = 'https://store.steampowered.com/app/'+str(app_id)+'/?l='+language\n",
    "                html = requests.get(url)\n",
    "                html_string = str(html.content)\n",
    "                raw_comment_count = re.split('<span class=\"user_reviews_count\">|</span> <a class=\"tooltip\" data-tooltip-html=', html_string)[-2]\n",
    "                formatted_comment_count = int(raw_comment_count.replace(',', '').strip('()'))\n",
    "                # Some languages display comment counts that are ADDED TO the EN count.\n",
    "                # Let's remove the EN value from them before moving on.\n",
    "                if language in languages_counted_with_english :\n",
    "                    formatted_comment_count = formatted_comment_count - en_comment_counts_by_app_id[app_id]\n",
    "                single_app_comment_languages[language] = formatted_comment_count\n",
    "\n",
    "            # If both fail, then it's a loss.\n",
    "            except:\n",
    "                single_app_comment_languages[language] = np.nan\n",
    "        \n",
    "        # Additional cleaning...\n",
    "        try :\n",
    "            # If the code block doens't parse, the var ends up null.\n",
    "            # Postgres disapproves.\n",
    "            if single_app_comment_languages[language] = None :\n",
    "                single_app_comment_languages[language] = 0\n",
    "            # Comment counts might end up negative because we performed\n",
    "            # subtraction on some of them, but the values we used for\n",
    "            # subtraction were performed at slightly different times.\n",
    "            # These errors, on review, are not consequential and can be\n",
    "            # swept under the rug.\n",
    "            if single_app_comment_languages[language] < 0 :\n",
    "                single_app_comment_langauges[language] = 0\n",
    "        except :\n",
    "            # If something else went wrong, I need to know.\n",
    "            # This will cause the table to fail to ingest, so I can check.\n",
    "            single_app_comment_languages[language] = 'ERROR'\n",
    "\n",
    "    # Rinse and repeat.\n",
    "    app_comment_languages.append(single_app_comment_languages)\n",
    "\n",
    "    interval = 1 + random.random() * 0.5\n",
    "    time.sleep(interval)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 27 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   app_id      20 non-null     int64\n",
      " 1   schinese    20 non-null     int64\n",
      " 2   tchinese    20 non-null     int64\n",
      " 3   japanese    20 non-null     int64\n",
      " 4   koreana     20 non-null     int64\n",
      " 5   thai        20 non-null     int64\n",
      " 6   bulgarian   20 non-null     int64\n",
      " 7   czech       20 non-null     int64\n",
      " 8   danish      20 non-null     int64\n",
      " 9   german      20 non-null     int64\n",
      " 10  spanish     20 non-null     int64\n",
      " 11  latam       20 non-null     int64\n",
      " 12  greek       20 non-null     int64\n",
      " 13  french      20 non-null     int64\n",
      " 14  italian     20 non-null     int64\n",
      " 15  indonesian  20 non-null     int64\n",
      " 16  hungarian   20 non-null     int64\n",
      " 17  dutch       20 non-null     int64\n",
      " 18  norwegian   20 non-null     int64\n",
      " 19  polish      20 non-null     int64\n",
      " 20  brazilian   20 non-null     int64\n",
      " 21  romanian    20 non-null     int64\n",
      " 22  russian     20 non-null     int64\n",
      " 23  finnish     20 non-null     int64\n",
      " 24  swedish     20 non-null     int64\n",
      " 25  turkish     20 non-null     int64\n",
      " 26  vietnamese  20 non-null     int64\n",
      "dtypes: int64(27)\n",
      "memory usage: 4.3 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Now we iterate over that function for all app ids.\n",
    "# I'm also resetting the dic/list variables here since I ran these cells out of order a lot\n",
    "# during bugfixing.\n",
    "app_comment_languages = []\n",
    "single_app_comment_languages = {}\n",
    "\n",
    "# Pass each app_id into the function along with our list of target languages.\n",
    "for index, row in scraped_search_results_df.iterrows() :\n",
    "    comments_in_all_languages(row['app_id'], all_counted_non_english_languages)\n",
    "\n",
    "# Export because I'm risk-averse.\n",
    "comment_languages_df = pd.DataFrame(app_comment_languages)\n",
    "comment_languages_df.to_json('../data/raw/Comment Languages DF.json', orient='records')\n",
    "\n",
    "# Peek peek\n",
    "print(comment_languages_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we merge our separate dfs.\n",
    "games_df = pd.merge(joined_games_df, comment_languages_df, on=\"app_id\", how='inner')\n",
    "\n",
    "if os.path.exists('../data/raw/0 - Scraped Games DF.json') :\n",
    "    existing_records = pd.read_json('../data/raw/0 - Scraped Games DF.json', orient='records')\n",
    "    brand_new_fancy_updated_version = pd.concat([existing_records, games_df], axis=0, ignore_index=True)\n",
    "    brand_new_fancy_updated_version.to_json('../data/raw/0 - Scraped Games DF.json', orient='records')\n",
    "    \n",
    "else :\n",
    "    games_df.to_json('../data/raw/0 - Scraped Games DF.json', orient='records')\n",
    "\n",
    "# Data scraped!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
