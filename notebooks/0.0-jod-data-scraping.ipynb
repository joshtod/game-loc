{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Video game localizion prioritization tool proposal\n",
    "---\n",
    "\n",
    "My goal is to scrape and analyze data from the video game platform\n",
    "Steam in order to help studios or localization service providers\n",
    "choose which languages they should localize into in order to\n",
    "maximize their localization ROI.\n",
    "\n",
    "The dataset would consist of a database of games with columns including\n",
    "genre, sales, price, number of reviews, percent of positive reviews,\n",
    "available languages, and the language that positive or negative reviews\n",
    "are written in. Any relationships between these variables (especially\n",
    "between language, genre, sales, price, and positive reviews, if such a\n",
    "relationship is found) could be instrumental in driving business\n",
    "decisions on the studio or language service provider level.\n",
    "\n",
    "The code below is the beginning of my scraper. It scrapes a search result\n",
    "page to gather the name, price, number of reviews, percent of positive\n",
    "reviews, and individual game page url for all listed games. In order to\n",
    "suit the needs of my project, it must be expanded to also perform the\n",
    "following:\n",
    "\n",
    "1. Scroll through a results list in order to cause the page to load more\n",
    "results (current max is 50). Tools exist for this, but I haven't had the\n",
    "time to study them yet.\n",
    "\n",
    "2. Perform a secondary scraping of the individual games' pages to collect\n",
    "the remainder of the column info that I haven't scraped yet. This is\n",
    "theoretically possible with my current limited skillset, though I worry\n",
    "that so many rapid calls will cause Steam to ban my ISP, so I should also\n",
    "study tools that slow down and/or randomize the request timing.\n",
    "\n",
    "3. Be able to ascertain the language in which a review is written. I think\n",
    "there are tools available for this - worst case scenario, I just ask\n",
    "ChatGPT 3.5 which language it is, using a rotating cast of ISPs to bypass\n",
    "the daily message limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DS stuff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Trying not to get blocked while scraping by inputting\n",
    "# random delays between Get requests.\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import chardet\n",
    "\n",
    "# I needed some extra help locating specific parts within a\n",
    "# bs4 tag object, so I got this.\n",
    "import re\n",
    "\n",
    "# For file tracking when exporting files.\n",
    "from datetime import date\n",
    "\n",
    "# I didn't end up using this one, but that might be because\n",
    "# I still have no idea what the eff I'm doing. Leaving it for\n",
    "# now in case I need it later.\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Learn about the page\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE ONLY WORKS IF YOUR STEAM SETTINGS ARE SET TO PAGINATED\n",
    "# SEARCH RESULTS, NOT INFINITE SCROLL.\n",
    "\n",
    "# This url is for the \"all products\" search with the result type\n",
    "# limited to \"Games\" (category1=998)\n",
    "url = \"https://store.steampowered.com/search/?category1=998\"\n",
    "html = urlopen(url)\n",
    "current_page_soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"search_result_row ds_collapse_flag\" data-ds-appid=\"1086940\" data-ds-crtrids=\"[6879350]\" data-ds-descids=\"[1,2,5]\" data-ds-itemkey=\"App_1086940\" data-ds-steam-deck-compat-handled=\"true\" data-ds-tagids=\"[122,6426,1742,4747,21,4474,3843]\" data-gpnav=\"item\" data-search-page=\"1\" href=\"https://store.steampowered.com/app/1086940/Baldurs_Gate_3/?snr=1_7_7_230_150_1\" onmouseout=\"HideGameHover( this, event, 'global_hover' )\" onmouseover=\"GameHover( this, event, 'global_hover', {&quot;type&quot;:&quot;app&quot;,&quot;id&quot;:1086940,&quot;public&quot;:1,&quot;v6&quot;:1} );\">\n",
      " <div class=\"col search_capsule\">\n",
      "  <img src=\"https://cdn.cloudflare.steamstatic.com/steam/apps/1086940/capsule_sm_120.jpg?t=1692294127\" srcset=\"https://cdn.cloudflare.steamstatic.com/steam/apps/1086940/capsule_sm_120.jpg?t=1692294127 1x, https://cdn.cloudflare.steamstatic.com/steam/apps/1086940/capsule_231x87.jpg?t=1692294127 2x\"/>\n",
      " </div>\n",
      " <div class=\"responsive_search_name_combined\">\n",
      "  <div class=\"col search_name ellipsis\">\n",
      "   <span class=\"title\">\n",
      "    Baldur's Gate 3\n",
      "   </span>\n",
      "   <div>\n",
      "    <span class=\"platform_img win\">\n",
      "    </span>\n",
      "    <span class=\"platform_img mac\">\n",
      "    </span>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"col search_released responsive_secondrow\">\n",
      "   Aug 3, 2023\n",
      "  </div>\n",
      "  <div class=\"col search_reviewscore responsive_secondrow\">\n",
      "   <span class=\"search_review_summary positive\" data-tooltip-html=\"Overwhelmingly Positive&lt;br&gt;95% of the 259,208 user reviews for this game are positive.\">\n",
      "   </span>\n",
      "  </div>\n",
      "  <div class=\"col search_price_discount_combined responsive_secondrow\" data-price-final=\"5999\">\n",
      "   <div class=\"col search_discount_and_price responsive_secondrow\">\n",
      "    <div class=\"discount_block search_discount_block no_discount\" data-bundlediscount=\"0\" data-discount=\"0\" data-price-final=\"5999\">\n",
      "     <div class=\"discount_prices\">\n",
      "      <div class=\"discount_final_price\">\n",
      "       $59.99\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      " </div>\n",
      " <div style=\"clear: left;\">\n",
      " </div>\n",
      "</a>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From looking at the whole page's HTML, I can tell which tag to call in order\n",
    "# to get the information relevant to only a single game.\n",
    "\n",
    "single_game_example = current_page_soup.find('a', class_='search_result_row ds_collapse_flag')\n",
    "\n",
    "print(single_game_example.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I learned the hard way that not all listings are identical. Most listings are for 'app's, but\n",
    "# some are for 'bundle's. Bundles have a slightly different leading 'a' tag - not different enough\n",
    "# that we need to use a different attribute to access them, but different enough that we need to\n",
    "# use different attributes to scrape some of the data.\n",
    "\n",
    "# Let's pull one up for reference.\n",
    "\n",
    "for listing in current_page_soup.find_all('a', class_='search_result_row ds_collapse_flag') :\n",
    "    if listing.has_attr('data-ds-bundle-data') :\n",
    "        print(listing.prettify())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Scrape the first set of data from the search results pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know what our soups will look like, we can write functions to do the scraping.\n",
    "# The first function will scrape all the relevant data off of the current results page.\n",
    "# The second function will programmatically switch to the next page of results.\n",
    "# Later, we will run both functions within a loop in order to scrape all results data\n",
    "# from all pages.\n",
    "\n",
    "# This is only the first round of scraping. Later, we will scrape more data from each\n",
    "# game's store page. Since that process is completely different, we will define new\n",
    "# functions for it later, after this round of scraping is complete.\n",
    "\n",
    "# Loop through the HTML blocks for each game and scrape the key info into a dictionary,\n",
    "# then add the dictionaries to the list.\n",
    "# I'm not cleaning up the data types at this point - I'm learning as I'm going, so I'm\n",
    "# prioritizing getting all the info I need into the df, and then working with data\n",
    "# types later either by doing operations on the df or re-writing some of this code.\n",
    "def scrape_current_page(current_page_soup) :\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes the soup of a paginated Steam search results page (NOT infinte scroll)\n",
    "    and scrapes the:\n",
    "    \n",
    "    title\n",
    "    release_date\n",
    "    positive_review_percent\n",
    "    number_of_reviews\n",
    "    price\n",
    "    game_page_link\n",
    "    type\n",
    "    app_id\n",
    "    \n",
    "    from every game on the page. It puts these values into dictionaries and appends them to\n",
    "    the list called \"games\". \n",
    "    \"\"\"\n",
    "\n",
    "    for listing in current_page_soup.find_all('a', class_='search_result_row ds_collapse_flag') :\n",
    "\n",
    "        # Create (or clean out) an empty dictionary to hold the new info.\n",
    "        game = {}\n",
    "\n",
    "        # Listings on results pages can be one of two types - standalone games, or bundles.\n",
    "        # We only want to work with standalone games.\n",
    "        # Only apps have this tag in their listing.\n",
    "        if listing.has_attr('data-ds-appid') :\n",
    "            game['app_id'] = listing.get('data-ds-appid')\n",
    "\n",
    "            # The title and release date seem to be at uniform locations in all listings.\n",
    "            game['title'] = listing.find('span', class_='title').get_text()\n",
    "            game['release_date'] = listing.find('div', class_='col search_released responsive_secondrow').get_text()\n",
    "\n",
    "            # Not all games have reviws listed, so we have to account for code blocks that omit this part.\n",
    "            # I might eventually remove this part and scrape the review data from the individual game pages\n",
    "            # instead, since it seems to be more complete there. This is just proof of concept for now.\n",
    "            try:\n",
    "                review_string = re.split('>| of|the | user', listing.find('div', class_='col search_reviewscore responsive_secondrow') \\\n",
    "                                                            .find('span').get('data-tooltip-html'))\n",
    "                game['positive_review_percent'] = review_string[1]\n",
    "                game['number_of_reviews'] = review_string[3]\n",
    "            except: \n",
    "                game['positive_review_percent'] = np.nan\n",
    "                game['number_of_reviews'] = np.nan\n",
    "            \n",
    "            # Same for price - many unreleased games do not have price info, so we have to skip them.\n",
    "            # Some games have an original price and a discounted price listed, but for the time being\n",
    "            # I've decided to only go by original prices, so I'll default to that and only return\n",
    "            # a null value if no kind of price whatsoever is listed.\n",
    "            try: \n",
    "                game['price'] = listing.find('div', class_=\"discount_original_price\").get_text()\n",
    "            except:\n",
    "                try:\n",
    "                    game['price'] = listing.find('div', class_=\"discount_final_price\").get_text()\n",
    "                except:\n",
    "                    game['price'] = np.nan\n",
    "\n",
    "            # Weirdly enough, not every game seems to have its own page.\n",
    "            try:\n",
    "                game['game_page_link'] = listing.get('href')\n",
    "            except:\n",
    "                game['game_page_link'] = 'Failed'\n",
    "\n",
    "            # Now we grab the tags, which will be a major feature in our analysis.\n",
    "            try :\n",
    "                game['tags'] = listing.get('data-ds-tagids')\n",
    "            except :\n",
    "                game['tags'] = 'Failed'\n",
    "\n",
    "            # Now we add this dict to the list, rinse and repeat.\n",
    "            games.append(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the function that determines if there is a next page of\n",
    "# results, or if we're already at the last page.\n",
    "\n",
    "def get_next_page_url(current_page_soup) :\n",
    "\n",
    "        \"\"\"\n",
    "        This function takes the soup of a paginated Steam search results page (NOT infinte scroll)\n",
    "        and determines whether it is the last page of results.\n",
    "\n",
    "        If it is not the last page, the URL of the next page is stored in \"next_link\".\n",
    "\n",
    "        If it is the last page, \"next_link\" will be set to False.\n",
    "        \"\"\"\n",
    "\n",
    "        # First, we check to make sure there IS a next page. We can tell by looking\n",
    "        # at the 'pagebtn' tags.\n",
    "        pagebtn_tags = current_page_soup.find_all('a', class_='pagebtn')\n",
    "\n",
    "        # This is the variable that we will use to store the next link, or set it to\n",
    "        # False to let the loop know that we're done scraping.\n",
    "        global next_link\n",
    "\n",
    "        # If it is any of the middle pages, there will be two pagebtn tags.\n",
    "        # The link we need is in side the pagebtn tag that displays the text '>'.\n",
    "        if len(pagebtn_tags) == 2 :\n",
    "                next_link = pagebtn_tags[1].get('href')\n",
    "\n",
    "        # If there is only one pagebtn tag, that means we're on the first page or the \n",
    "        # last page. If it's the first page, then the pagebtn tag will contain the\n",
    "        # character '>'.\n",
    "        elif pagebtn_tags[0].get_text() == \">\" :\n",
    "                next_link = pagebtn_tags[0].get('href')\n",
    "\n",
    "        # If neither of the above conditions are met, then we're on the last page and\n",
    "        # we can set \"next_link\" to False, triggering the loop to stop scraping.\n",
    "        else :\n",
    "                next_link = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n"
     ]
    }
   ],
   "source": [
    "# Now that we have our functions, we'll iterate over them to scrape the data.\n",
    "\n",
    "# Set the first url to be processed to the first page of search results.\n",
    "next_link = url\n",
    "\n",
    "# Create the list that will hold the dictionaries of game info.\n",
    "games = []\n",
    "\n",
    "# Now we decide how many results we want. \n",
    "# \n",
    "# The main constraint here is time - since\n",
    "# we don't want to get IP banned, we'll have set delay between each get request.\n",
    "# This isn't so important for this loop, since we can get 25 games in one get request.\n",
    "# However, later we'll be going through the games' pages one-by-one, and in some cases\n",
    "# we'll have to do 10 different get requests per game to scrape language-specific data.\n",
    "# Therefore, adding 1 game adds at least 11 get requests & delays to our process.\n",
    "# (I ended up scraping for over 10 hours.)\n",
    "#\n",
    "# Will only limit to inteverals of 25 (as there are 25 results per page).\n",
    "# If games_to_scrape is greater than the number of games in the search results, then\n",
    "# the the will automatically stop trying to scrape when it reaches the end of the\n",
    "# final page of results, because get_next_page_url will set the next_link variable to False.\n",
    "#\n",
    "# I want to play with a set of about 3,000 games, but some will be unusable or duplicated,\n",
    "# so let's overshoot and just play with what we get. \n",
    "games_to_scrape = 3200\n",
    "\n",
    "# Now, loop. Keep scraping as long as our games list is shorter than the games_to_scrape var.\n",
    "while len(games) < games_to_scrape :\n",
    "    \n",
    "    # Soup up the page in question.\n",
    "    html = urlopen(next_link)\n",
    "    current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Scrape that page.\n",
    "    scrape_current_page(current_page_soup)\n",
    "\n",
    "    # Set \"next_link\" to the next URL we want to scrape.\n",
    "    get_next_page_url(current_page_soup)\n",
    "\n",
    "    # Include a random delay to prevent getting IP blocked.\n",
    "    interval = 1.5 + random.random() * 0.5\n",
    "    time.sleep(interval)\n",
    "\n",
    "    if next_link == False :\n",
    "        print('Fewer than '+str(games_to_scrape)+' games in the search results.')\n",
    "        print(str(len(games))+' games scraped.')\n",
    "        break\n",
    "\n",
    "# Check our work!\n",
    "print(len(games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3171 entries, 0 to 3170\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   app_id                   3171 non-null   object\n",
      " 1   title                    3171 non-null   object\n",
      " 2   release_date             3171 non-null   object\n",
      " 3   positive_review_percent  3145 non-null   object\n",
      " 4   number_of_reviews        3145 non-null   object\n",
      " 5   price                    2905 non-null   object\n",
      " 6   game_page_link           3171 non-null   object\n",
      " 7   tags                     3171 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 198.3+ KB\n",
      "None\n",
      "    app_id                               title  release_date  \\\n",
      "0  1086940                     Baldur's Gate 3   Aug 3, 2023   \n",
      "1      730    Counter-Strike: Global Offensive  Aug 21, 2012   \n",
      "2  1888160  ARMORED CORE™ VI FIRES OF RUBICON™  Aug 24, 2023   \n",
      "3  1085660                           Destiny 2   Oct 1, 2019   \n",
      "4  1172470                       Apex Legends™   Nov 4, 2020   \n",
      "\n",
      "  positive_review_percent number_of_reviews   price  \\\n",
      "0                     95%           259,208  $59.99   \n",
      "1                     88%         7,482,669  $14.99   \n",
      "2                     86%            29,497  $59.99   \n",
      "3                     81%           562,194     NaN   \n",
      "4                     80%           711,652     NaN   \n",
      "\n",
      "                                      game_page_link  \\\n",
      "0  https://store.steampowered.com/app/1086940/Bal...   \n",
      "1  https://store.steampowered.com/app/730/Counter...   \n",
      "2  https://store.steampowered.com/app/1888160/ARM...   \n",
      "3  https://store.steampowered.com/app/1085660/Des...   \n",
      "4  https://store.steampowered.com/app/1172470/Ape...   \n",
      "\n",
      "                                    tags  \n",
      "0      [122,6426,1742,4747,21,4474,3843]  \n",
      "1     [1663,1774,3859,3878,19,5711,5055]  \n",
      "2     [4821,4747,19,1697,3993,4191,5752]  \n",
      "3  [113,1695,1663,353880,1754,1685,1775]  \n",
      "4  [113,3859,176981,1774,1663,3839,1775]  \n"
     ]
    }
   ],
   "source": [
    "# Frame it and check.\n",
    "scraped_search_results_df = pd.DataFrame(games)\n",
    "\n",
    "# This results in some duplicates - sometimes different versions of the game have the same app id.\n",
    "# Because we're interested in the relative ration of comment frequencies, not in the total number\n",
    "# of games or total number of comments, we can safely drop duplicates even if they have different\n",
    "# comments.\n",
    "scraped_search_results_df = scraped_search_results_df.drop_duplicates(subset='app_id', keep='first')\n",
    "scraped_search_results_df = scraped_search_results_df.reset_index(drop=True)\n",
    "\n",
    "# We'll save this as a csv for convenience and because I don't trust %store yet.\n",
    "scraped_search_results_df.to_csv('../data/raw/Scraped Search Results.csv')\n",
    "\n",
    "# Peek peek.\n",
    "print(scraped_search_results_df.info())\n",
    "print(scraped_search_results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Scrape additional data for each game from its individual game page\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to use the URLs we just scraped to go through the pages\n",
    "# one-by-one and scrape more data.\n",
    "\n",
    "# We'll put all this data in a completely different df, then join them\n",
    "# when we're done on app_id.\n",
    "def scrape_game_page_data(current_page_soup) :\n",
    "\n",
    "    \"\"\"\n",
    "    This function scrapes info from all the individual games pages\n",
    "    currently referenced in games_info_df. We put the info in a dict\n",
    "    \"game\", then append it to \"games_extend_list\".\n",
    "    \n",
    "    Later, we will turn that list into another df and merge it to\n",
    "    games_info_df on index.\n",
    "\n",
    "    Scraped information is:\n",
    "\n",
    "    app_id\n",
    "    developer\n",
    "    publisher\n",
    "    description\n",
    "    interface_languages\n",
    "    full_audio_languages\n",
    "    subtitles_languages\n",
    "    english     <-- the number of user comments in English\n",
    "    \"\"\"\n",
    "    # For bugfixing\n",
    "    global touched_ids\n",
    "    \n",
    "    # Create/clear out the dictionary.\n",
    "    game = {}\n",
    "\n",
    "\n",
    "    # Weirdly, the best place to find the app_id is in the reflexive URL.\n",
    "    # If we split the url by slashes, the app id is third from the end.\n",
    "    try:\n",
    "        url_string = current_page_soup.find('link', rel='canonical').get('href')\n",
    "        url_string = re.split('/', url_string)\n",
    "        game[\"app_id\"] = url_string[-3]\n",
    "        touched_ids.append(game['app_id'])\n",
    "    except:\n",
    "        game[\"app_id\"] = \"Failed\"\n",
    "\n",
    "    # We can get the developer and publisher from the same code block.\n",
    "    try :\n",
    "        code_block = current_page_soup.find('div', attrs={'id':'appHeaderGridContainer'})\n",
    "    except :\n",
    "        pass\n",
    "\n",
    "    # The developer name is at a fixed location.\n",
    "    try:\n",
    "        game['developer'] = code_block.find('div', class_='grid_content').get_text()\n",
    "        # Don't know why it always brings in a newline at the beginning of the string. and a\n",
    "        # space at the end. Let's take those out.\n",
    "        game['developer'] = game['developer'][1:-1]\n",
    "    except :\n",
    "        game['developer'] = 'Failed'\n",
    "\n",
    "    # The publisher name is also at a fixed location. Not every game has a publisher, though.\n",
    "    try :\n",
    "        game['publisher'] = code_block.find('div', class_='grid_label', string='Publisher').find_next('a').get_text()\n",
    "    except :\n",
    "        game['publisher'] = 'None'\n",
    "\n",
    "    # Descriptions are at a fixed location.\n",
    "    try:\n",
    "        game['description'] = current_page_soup.find('meta', attrs={'name':'Description'}).get('content')\n",
    "    except :\n",
    "        game['description'] = 'Failed'\n",
    "\n",
    "    # The languages are listed as rows of a table.\n",
    "    # There are three different ways languages can be implemented in the game.\n",
    "    # As we look through the table, we'll store the languages in separate lists.\n",
    "    interface_languages = []\n",
    "    full_audio_languages = []\n",
    "    subtitles_languages = []\n",
    "    language_types = [interface_languages, full_audio_languages, subtitles_languages]\n",
    "\n",
    "    # The source code is compex so let's isolate the relevant block for safety.\n",
    "    try :\n",
    "        languages_code_block = current_page_soup.find('table', class_='game_language_options')\n",
    "    # I'll leave a note for myself to help with bugfixing if needed.\n",
    "    except :\n",
    "        language_types[0] = 'Did not find code block'\n",
    "\n",
    "    # Each \"row\" of the table is separated by a re tag. However, there's an extra\n",
    "    # tr tag at the beginning of languages_code_block that I couldn't find a better\n",
    "    # way to work around - since it has no text, it'll throw an error on .get_text,\n",
    "    # so we can just try/except our way out of it.\n",
    "    try :\n",
    "        for row in languages_code_block.find_all('tr', class_='') :\n",
    "            try :\n",
    "                current_language = row.find('td', class_='ellipsis').get_text()\n",
    "                # The text has a lot of formatting in it. No more!\n",
    "                current_language = re.sub('\\t|\\n|\\r', '', current_language)\n",
    "\n",
    "                # The code block represents each cell of the row with a td class='checkcol'\n",
    "                # tag. In order, the three cells of each row are interface, full audio,\n",
    "                # and subtitles. If the language of that row does not have one of those\n",
    "                # services, then there will be no more code inside the tags. If it does,\n",
    "                # then there will be a \"span\" tag in there along with a checkmark.\n",
    "\n",
    "                # Since the three types of language services are always in order,\n",
    "                # we can basically use 'counter' to iterate through the list of lists\n",
    "                # of language service types and only append the name of the language\n",
    "                # if that section of code has the \"span\" tag that indicates a checkmark.\n",
    "                counter = 0\n",
    "                for column in row.find_all('td', class_='checkcol') :\n",
    "                    if column.find('span') :\n",
    "                        language_types[counter].append(current_language)\n",
    "                    counter += 1\n",
    "            except :\n",
    "                pass\n",
    "    # For bugfixing.\n",
    "    except :\n",
    "        language_types[0] = 'Found code block, failed to parse within code block'\n",
    "\n",
    "\n",
    "    # Now we add the lists to our dictionary. We can access the lists via\n",
    "    # the index of the language_types list of lists.\n",
    "    game['interface_languages'] = language_types[0]\n",
    "    game['full_audio_languages'] = language_types[1]\n",
    "    game['subtitles_languages'] = language_types[2]\n",
    "\n",
    "    # I would love to have rating data available for the games, but Steam does not\n",
    "    # present it systematically (probably because so many games are not rated,\n",
    "    # and because there are different rating systems.)\n",
    "    # Maybe someday.\n",
    "    # game['rating'] = PG, Mature Audiences, etc...\n",
    "\n",
    "    # Now we get the number of reviews that are in English.\n",
    "    # To get the numbers for other languages, we'll have to modify the URL parameters\n",
    "    # and get the page again, so that'll be a big ol'loop that we'll do later.\n",
    "    try:\n",
    "        game['english'] = current_page_soup.find('label', attrs={'for':'review_language_mine'}) \\\n",
    "                                                            .find_next('span', class_='user_reviews_count').get_text()\n",
    "    except:\n",
    "        game['english'] = 0\n",
    "\n",
    "    # Rinse and repeat.\n",
    "    games_extend_list.append(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm declaring/cleaning out the list in a different cell because I hit a lot of \n",
    "# exceptions while testing this, and I didn't want to accidentally clean out all\n",
    "# my previous hard work each time I made a fix and continued the process. \n",
    "games_extend_list = []\n",
    "\n",
    "# Since running the following cell requires repeated get requests and sleep intervals,\n",
    "# and since many failures tend to happen 20 minutes or more into the process,\n",
    "# we can build in a ticker that keeps track of how far we got LAST time.\n",
    "# Then, after we debug, we can start right over from where we left off. \n",
    "ticker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bugfixing.\n",
    "touched_ids = []\n",
    "\n",
    "# Now we loop over all all app_ids in the df we created earlier.\n",
    "for index, row in scraped_search_results_df.iterrows() :\n",
    "    \n",
    "    # This is for bugfixing. If the loop throws an exception, I can use the ticker\n",
    "    # variable to quickly pick up where we left off.\n",
    "    if index == ticker :\n",
    "        # Soup up the page.\n",
    "        url = row['game_page_link']\n",
    "        html = urlopen(url)\n",
    "        current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Scrape the page.\n",
    "        scrape_game_page_data(current_page_soup)\n",
    "\n",
    "        # Include a random delay to prevent getting IP blocked.\n",
    "        interval = 1.5 + random.random() * 0.5\n",
    "        time.sleep(interval)\n",
    "        \n",
    "        # If the loop throws an exception on a game, 'ticker' will thus be equal\n",
    "        # to that game's index in the df, and I can go see what the problem was.\n",
    "        ticker = index + 1\n",
    "        \n",
    "\n",
    "# Turn the new list of dicts into a new df.\n",
    "scraped_game_pages_df = pd.DataFrame(games_extend_list)\n",
    "scraped_game_pages_df.to_csv('../data/raw/Scraped Game Pages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3154 entries, 0 to 3153\n",
      "Data columns (total 15 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   app_id                   3154 non-null   object\n",
      " 1   title                    3154 non-null   object\n",
      " 2   release_date             3154 non-null   object\n",
      " 3   positive_review_percent  3128 non-null   object\n",
      " 4   number_of_reviews        3128 non-null   object\n",
      " 5   price                    2889 non-null   object\n",
      " 6   game_page_link           3154 non-null   object\n",
      " 7   tags                     3154 non-null   object\n",
      " 8   developer                3154 non-null   object\n",
      " 9   publisher                3154 non-null   object\n",
      " 10  description              3154 non-null   object\n",
      " 11  interface_languages      3154 non-null   object\n",
      " 12  full_audio_languages     3154 non-null   object\n",
      " 13  subtitles_languages      3154 non-null   object\n",
      " 14  english                  3154 non-null   object\n",
      "dtypes: object(15)\n",
      "memory usage: 394.2+ KB\n",
      "None\n",
      "    app_id                               title  release_date  \\\n",
      "0  1086940                     Baldur's Gate 3   Aug 3, 2023   \n",
      "1      730    Counter-Strike: Global Offensive  Aug 21, 2012   \n",
      "2  1888160  ARMORED CORE™ VI FIRES OF RUBICON™  Aug 24, 2023   \n",
      "3  1085660                           Destiny 2   Oct 1, 2019   \n",
      "4  1172470                       Apex Legends™   Nov 4, 2020   \n",
      "\n",
      "  positive_review_percent number_of_reviews   price  \\\n",
      "0                     95%           259,208  $59.99   \n",
      "1                     88%         7,482,669  $14.99   \n",
      "2                     86%            29,497  $59.99   \n",
      "3                     81%           562,194     NaN   \n",
      "4                     80%           711,652     NaN   \n",
      "\n",
      "                                      game_page_link  \\\n",
      "0  https://store.steampowered.com/app/1086940/Bal...   \n",
      "1  https://store.steampowered.com/app/730/Counter...   \n",
      "2  https://store.steampowered.com/app/1888160/ARM...   \n",
      "3  https://store.steampowered.com/app/1085660/Des...   \n",
      "4  https://store.steampowered.com/app/1172470/Ape...   \n",
      "\n",
      "                                    tags                         developer  \\\n",
      "0      [122,6426,1742,4747,21,4474,3843]                    Larian Studios   \n",
      "1     [1663,1774,3859,3878,19,5711,5055]  Valve, Hidden Path Entertainment   \n",
      "2     [4821,4747,19,1697,3993,4191,5752]                 FromSoftware Inc.   \n",
      "3  [113,1695,1663,353880,1754,1685,1775]                            Bungie   \n",
      "4  [113,3859,176981,1774,1663,3839,1775]             Respawn Entertainment   \n",
      "\n",
      "           publisher                                        description  \\\n",
      "0     Larian Studios  Baldur’s Gate 3 is a story-rich, party-based R...   \n",
      "1              Valve  Counter-Strike: Global Offensive (CS: GO) expa...   \n",
      "2  FromSoftware Inc.  A new action game based on the concept of the ...   \n",
      "3            Bungie   Destiny 2 is an action MMO with a single evolv...   \n",
      "4    Electronic Arts  Apex Legends is the award-winning, free-to-pla...   \n",
      "\n",
      "                                 interface_languages  \\\n",
      "0  [English, French, German, Spanish - Spain, Pol...   \n",
      "1  [English, Czech, Danish, Dutch, Finnish, Frenc...   \n",
      "2  [English, French, Italian, German, Spanish - S...   \n",
      "3  [English, French, Italian, German, Spanish - S...   \n",
      "4  [English, French, Italian, German, Spanish - S...   \n",
      "\n",
      "                                full_audio_languages  \\\n",
      "0                                          [English]   \n",
      "1                                          [English]   \n",
      "2                                [English, Japanese]   \n",
      "3  [English, French, Italian, German, Spanish - S...   \n",
      "4  [English, French, Italian, German, Spanish - S...   \n",
      "\n",
      "                                 subtitles_languages      english  \n",
      "0  [English, French, German, Spanish - Spain, Pol...    (185,955)  \n",
      "1                                                 []  (2,061,835)  \n",
      "2  [English, French, Italian, German, Spanish - S...     (21,919)  \n",
      "3  [English, French, Italian, German, Spanish - S...    (324,241)  \n",
      "4                                                 []    (347,995)  \n"
     ]
    }
   ],
   "source": [
    "# Now we join our dataframes to create our core dataset.\n",
    "# I say \"core,\" even though our all-important label has yet to be scraped.\n",
    "# Bear with me. I'm new at this.\n",
    "joined_games_df = pd.merge(scraped_search_results_df, scraped_game_pages_df, on=\"app_id\", how='inner')\n",
    "joined_games_df.to_csv('../data/imterim/Joined Games DF.csv')\n",
    "print(joined_games_df.info())\n",
    "print(joined_games_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Scrape the number of comments in each language from the games' pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'top_10_languages' (list)\n"
     ]
    }
   ],
   "source": [
    "# Now we begin the task of getting all the comment counts for each different language.\n",
    "# Since this process requires a huge amount of get requests/time, we'll limit our exploration\n",
    "# to the 10 most common languages for game localization (assuming the source text is English).\n",
    "\n",
    "# Here's a list of all the language codes on Steam, for good measure.\n",
    "# Don't know if we'll use it, but here it is.\n",
    "all_languages = ['schinese', 'tchinese', 'japanese', 'koreana', 'thai', 'bulgarian', 'czech', 'danish', \\\n",
    "                 'german', 'english', 'spanish', 'latam', 'greek', 'french', 'italian', 'indonesian', \\\n",
    "                 'hungarian', 'dutch', 'norwegian', 'polish', 'portugese', 'brazilian', 'romanian', \\\n",
    "                 'russian', 'finnish', 'swedish', 'turkish', 'vietnamese', 'ukranian']\n",
    "\n",
    "# These are the generally-accepted top 10 languages to localize into from EN.\n",
    "# The count of EN comments is important for our analysis, but it's already in the df.\n",
    "# No idea why they put an a on the end of Korean.\n",
    "top_10_languages = ['german', 'french', 'spanish', 'brazilian', 'russian', 'italian', 'schinese', \\\n",
    "                    'japanese', 'koreana', 'polish']\n",
    "\n",
    "%store top_10_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build a function that will find the number of reviews in a given language for a given game.\n",
    "# This function will iterate through our df (using the first one, which is also the smallest, for\n",
    "# good measure), creating a new column for the language and filling the value with the number.\n",
    "app_comment_languages = []\n",
    "single_app_comment_languages = {}\n",
    "\n",
    "def comments_in_all_languages(app_id, languages) :\n",
    "    \"\"\"\n",
    "    Takes a Steam app id and a list of languages (as spelled in Steam's html)\n",
    "    and creates a dictionary, then appends that dictionary to a list.\n",
    "\n",
    "    Intended to be iterated over.\n",
    "\n",
    "    The first key in the dictionary is \"app id\", and the value is the app id.\n",
    "\n",
    "    The rest of the keys are the names of the languages, and the values are\n",
    "    the number of comments on that game/app's page that are in that language.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the dict is empty at the beginning of each loop.\n",
    "    single_app_comment_languages = {}\n",
    "    \n",
    "    # Store the app_id in the dict.\n",
    "    single_app_comment_languages['app_id'] = app_id\n",
    "\n",
    "    # Soup up the game's page in the current language.\n",
    "    for language in languages :\n",
    "        url = 'https://store.steampowered.com/app/'+app_id+'/?l='+language\n",
    "        html = urlopen(url)\n",
    "        current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # There are 2 types of game page source code, used on games with different language settings.\n",
    "        # We'll try the most common one first, then try to execute the other type if this throws an exception.\n",
    "        try :\n",
    "            single_app_comment_languages[language] = current_page_soup.find('label', attrs={'for':'review_language_mine'}) \\\n",
    "                                                                        .find_next('span').get_text()\n",
    "        \n",
    "        # If that's no good, we try scraping the other way.\n",
    "        # The 'other way' can't be scraped effectively by urlopen(), so we'll use requests.get() instead.\n",
    "        except :\n",
    "            try :\n",
    "                url = 'https://store.steampowered.com/app/'+app_id+'/?l='+language\n",
    "                html = requests.get(url)\n",
    "                html_string = str(html.content)\n",
    "\n",
    "                single_app_comment_languages[language] = re.split('<span class=\"user_reviews_count\">|</span> <a class=\"tooltip\" data-tooltip-html=', html_string)[-2]\n",
    "\n",
    "            # If both fail, then it's a loss.\n",
    "            except:\n",
    "                single_app_comment_languages[language] = 'Failed'\n",
    "                \n",
    "        interval = 1.5 + random.random() * 0.5\n",
    "        time.sleep(interval)\n",
    "\n",
    "    # Rinse and repeat.\n",
    "    app_comment_languages.append(single_app_comment_languages)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3171 entries, 0 to 3170\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   app_id     3171 non-null   object\n",
      " 1   german     3171 non-null   object\n",
      " 2   french     3171 non-null   object\n",
      " 3   spanish    3171 non-null   object\n",
      " 4   brazilian  3171 non-null   object\n",
      " 5   russian    3171 non-null   object\n",
      " 6   italian    3171 non-null   object\n",
      " 7   schinese   3171 non-null   object\n",
      " 8   japanese   3171 non-null   object\n",
      " 9   koreana    3171 non-null   object\n",
      " 10  polish     3171 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 272.6+ KB\n",
      "None\n",
      "    app_id       german     french    spanish  brazilian      russian  \\\n",
      "0  1086940    (196,574)    (7,174)    (5,780)    (7,374)     (17,432)   \n",
      "1      730  (2,267,349)  (124,244)  (277,384)  (433,297)  (1,976,542)   \n",
      "2  1888160     (22,412)      (378)      (391)      (228)        (609)   \n",
      "3  1085660    (341,920)   (10,917)   (17,154)   (17,647)     (39,239)   \n",
      "4  1172470    (365,479)   (15,472)   (23,684)   (13,900)     (85,332)   \n",
      "\n",
      "    italian   schinese japanese   koreana     polish  \n",
      "0   (2,502)   (30,839)     (99)   (3,237)    (3,293)  \n",
      "1  (19,691)  (947,071)  (8,933)  (21,007)  (423,371)  \n",
      "2     (129)    (5,653)  (3,107)   (1,850)       (90)  \n",
      "3   (3,607)   (87,572)    (875)  (13,355)    (6,208)  \n",
      "4   (3,208)  (102,293)  (7,259)  (14,868)   (11,794)  \n"
     ]
    }
   ],
   "source": [
    "# Now we iterate over that function for all app ids.\n",
    "# I'm also resetting the dic/list variables here since I ran these cells out of order a lot\n",
    "# during bugfixing.\n",
    "app_comment_languages = []\n",
    "single_app_comment_languages = {}\n",
    "\n",
    "# Pass each app_id into the function along with our list of target languages.\n",
    "for index, row in scraped_search_results_df.iterrows() :\n",
    "    comments_in_all_languages(row['app_id'], top_10_languages)\n",
    "\n",
    "# Save as a .csv because I'm risk-averse.\n",
    "comment_languages_df = pd.DataFrame(app_comment_languages)\n",
    "comment_languages_df.to_csv('../data/raw/Comment Languages DF.csv')\n",
    "\n",
    "# Peek peek\n",
    "print(comment_languages_df.info())\n",
    "print(comment_languages_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3154 entries, 0 to 3153\n",
      "Data columns (total 25 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   app_id                   3154 non-null   object\n",
      " 1   title                    3154 non-null   object\n",
      " 2   release_date             3154 non-null   object\n",
      " 3   positive_review_percent  3128 non-null   object\n",
      " 4   number_of_reviews        3128 non-null   object\n",
      " 5   price                    2889 non-null   object\n",
      " 6   game_page_link           3154 non-null   object\n",
      " 7   tags                     3154 non-null   object\n",
      " 8   developer                3154 non-null   object\n",
      " 9   publisher                3154 non-null   object\n",
      " 10  description              3154 non-null   object\n",
      " 11  interface_languages      3154 non-null   object\n",
      " 12  full_audio_languages     3154 non-null   object\n",
      " 13  subtitles_languages      3154 non-null   object\n",
      " 14  english                  3154 non-null   object\n",
      " 15  german                   3154 non-null   object\n",
      " 16  french                   3154 non-null   object\n",
      " 17  spanish                  3154 non-null   object\n",
      " 18  brazilian                3154 non-null   object\n",
      " 19  russian                  3154 non-null   object\n",
      " 20  italian                  3154 non-null   object\n",
      " 21  schinese                 3154 non-null   object\n",
      " 22  japanese                 3154 non-null   object\n",
      " 23  koreana                  3154 non-null   object\n",
      " 24  polish                   3154 non-null   object\n",
      "dtypes: object(25)\n",
      "memory usage: 640.7+ KB\n",
      "None\n",
      "    app_id                               title  release_date  \\\n",
      "0  1086940                     Baldur's Gate 3   Aug 3, 2023   \n",
      "1      730    Counter-Strike: Global Offensive  Aug 21, 2012   \n",
      "2  1888160  ARMORED CORE™ VI FIRES OF RUBICON™  Aug 24, 2023   \n",
      "3  1085660                           Destiny 2   Oct 1, 2019   \n",
      "4  1172470                       Apex Legends™   Nov 4, 2020   \n",
      "\n",
      "  positive_review_percent number_of_reviews   price  \\\n",
      "0                     95%           259,208  $59.99   \n",
      "1                     88%         7,482,669  $14.99   \n",
      "2                     86%            29,497  $59.99   \n",
      "3                     81%           562,194     NaN   \n",
      "4                     80%           711,652     NaN   \n",
      "\n",
      "                                      game_page_link  \\\n",
      "0  https://store.steampowered.com/app/1086940/Bal...   \n",
      "1  https://store.steampowered.com/app/730/Counter...   \n",
      "2  https://store.steampowered.com/app/1888160/ARM...   \n",
      "3  https://store.steampowered.com/app/1085660/Des...   \n",
      "4  https://store.steampowered.com/app/1172470/Ape...   \n",
      "\n",
      "                                    tags                         developer  \\\n",
      "0      [122,6426,1742,4747,21,4474,3843]                    Larian Studios   \n",
      "1     [1663,1774,3859,3878,19,5711,5055]  Valve, Hidden Path Entertainment   \n",
      "2     [4821,4747,19,1697,3993,4191,5752]                 FromSoftware Inc.   \n",
      "3  [113,1695,1663,353880,1754,1685,1775]                            Bungie   \n",
      "4  [113,3859,176981,1774,1663,3839,1775]             Respawn Entertainment   \n",
      "\n",
      "           publisher  ...       german     french    spanish  brazilian  \\\n",
      "0     Larian Studios  ...    (196,574)    (7,174)    (5,780)    (7,374)   \n",
      "1              Valve  ...  (2,267,349)  (124,244)  (277,384)  (433,297)   \n",
      "2  FromSoftware Inc.  ...     (22,412)      (378)      (391)      (228)   \n",
      "3            Bungie   ...    (341,920)   (10,917)   (17,154)   (17,647)   \n",
      "4    Electronic Arts  ...    (365,479)   (15,472)   (23,684)   (13,900)   \n",
      "\n",
      "       russian   italian   schinese japanese   koreana     polish  \n",
      "0     (17,432)   (2,502)   (30,839)     (99)   (3,237)    (3,293)  \n",
      "1  (1,976,542)  (19,691)  (947,071)  (8,933)  (21,007)  (423,371)  \n",
      "2        (609)     (129)    (5,653)  (3,107)   (1,850)       (90)  \n",
      "3     (39,239)   (3,607)   (87,572)    (875)  (13,355)    (6,208)  \n",
      "4     (85,332)   (3,208)  (102,293)  (7,259)  (14,868)   (11,794)  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now we merge our dfs into our big main one.\n",
    "games_df = pd.merge(joined_games_df, comment_languages_df, on=\"app_id\", how='inner')\n",
    "games_df.to_csv('0 - Raw Scraped Games DF.csv')\n",
    "print(games_df.info())\n",
    "print(games_df.head())\n",
    "\n",
    "# Data scraped!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
